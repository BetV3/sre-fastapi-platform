# Prometheus Alert Rules
groups:
  # =============================================================================
  # Application Alerts
  # =============================================================================
  - name: application
    rules:
      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High HTTP error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"

      # High Latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
          > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value | humanizeDuration }}"

      # Application Down
      - alert: ApplicationDown
        expr: up{job="fastapi"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "FastAPI application is down"
          description: "The FastAPI application has been down for more than 1 minute"

      # Low Request Rate (potential issue)
      - alert: LowRequestRate
        expr: sum(rate(http_requests_total[5m])) < 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Unusually low request rate"
          description: "Request rate has dropped below 0.1 req/s for 10 minutes"

  # =============================================================================
  # Infrastructure Alerts
  # =============================================================================
  - name: infrastructure
    rules:
      # Prometheus Target Down
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target is down"
          description: "Target {{ $labels.job }} on {{ $labels.instance }} is down"

      # High Memory Usage (requires node_exporter)
      # - alert: HighMemoryUsage
      #   expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
      #   for: 5m
      #   labels:
      #     severity: warning
      #   annotations:
      #     summary: "High memory usage detected"
      #     description: "Memory usage is above 90%"

  # =============================================================================
  # Loki Alerts
  # =============================================================================
  - name: loki
    rules:
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Loki is down"
          description: "Loki logging service has been down for more than 2 minutes"

  # =============================================================================
  # Alertmanager Alerts
  # =============================================================================
  - name: alertmanager
    rules:
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Alertmanager is down"
          description: "Alertmanager has been down for more than 2 minutes"
